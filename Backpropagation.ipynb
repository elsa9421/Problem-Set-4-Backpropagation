{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elmathew_71827909",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f26be21e6f0f497c963a9122c92a09ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d145abe2dd734159ac5d0998b350b663",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_894f6d06c5b54c9d8d344aa6f8a13860",
              "IPY_MODEL_4e0f416f9d344ebf9c507b19d9333d0e"
            ]
          }
        },
        "d145abe2dd734159ac5d0998b350b663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "894f6d06c5b54c9d8d344aa6f8a13860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_28cb3214c1de4173a3416bacebad2676",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b175de219a242c4b1a87bbb5f2b87fa"
          }
        },
        "4e0f416f9d344ebf9c507b19d9333d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_50a4e4d64d204d43b8e8a772cce6960a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:03, 43235137.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e8c5b0e98974f288b01ba82e3ad7b59"
          }
        },
        "28cb3214c1de4173a3416bacebad2676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b175de219a242c4b1a87bbb5f2b87fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50a4e4d64d204d43b8e8a772cce6960a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e8c5b0e98974f288b01ba82e3ad7b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n",
        "Please provide the following information \n",
        "(e.g. Andrew Owens, ahowens):\n",
        "\n",
        "Elsa Mary Mathew, elmathew\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "outputId": "73b3e161-7b9d-493d-9675-8f9d67557a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "f26be21e6f0f497c963a9122c92a09ff",
            "d145abe2dd734159ac5d0998b350b663",
            "894f6d06c5b54c9d8d344aa6f8a13860",
            "4e0f416f9d344ebf9c507b19d9333d0e",
            "28cb3214c1de4173a3416bacebad2676",
            "3b175de219a242c4b1a87bbb5f2b87fa",
            "50a4e4d64d204d43b8e8a772cce6960a",
            "6e8c5b0e98974f288b01ba82e3ad7b59"
          ]
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f26be21e6f0f497c963a9122c92a09ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing in put data, of shape (N,  Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    \n",
        "    out = np.dot(x, w) + b\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################\n",
        "    N,Din=x.shape\n",
        "    dx = np.dot(dout,w.T)\n",
        "    dw = np.dot(x.T,dout)\n",
        "    db = np.dot(np.ones((N,)),dout)\n",
        "\n",
        "\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx,dw,db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    #out= np.empty(np.shape(x))\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    out=np.maximum(0,x)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "\n",
        "    y=(x>0)\n",
        "    x=y.astype(int)\n",
        "    dx=np.multiply(dout,x)\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    \n",
        "  \n",
        "\n",
        "    N,C=x.shape\n",
        "    y_predicted=np.exp(x-np.max(x, axis = 1,keepdims = True))/np.sum(np.exp(x-np.max(x, axis = 1,keepdims = True)),axis=1,keepdims=True)\n",
        "    loss=-np.sum(np.log(y_predicted[range(N),y]))/N\n",
        "    gradient = y_predicted.copy()\n",
        "    gradient[range(N),y] -= 1\n",
        "    gradient= gradient/N\n",
        "    dx = gradient\n",
        "      \n",
        "    \n",
        " \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        self.params['W1'] = np.random.normal(scale=weight_scale, size=(input_dim, hidden_dim))\n",
        "        self.params['W2'] = np.random.normal(scale=weight_scale, size=(hidden_dim, num_classes))\n",
        "        self.params['b1'] = np.zeros(hidden_dim)\n",
        "        self.params['b2'] = np.zeros(num_classes)\n",
        "   \n",
        "\n",
        "       \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "        #   1) input, 2) fully connected layer, 3) ReLU, 4) fully connected layer, 5) softmax.\n",
        "         #More concretely, we compute class probabilities c from an input image x as:\n",
        "         # c = softmax(W2 relu(W1x + b1) + b2)\n",
        "\n",
        "        Layer1,cache1=  fc_forward(X, self.params['W1'], self.params['b1'])\n",
        "        ReLu_layer,cache_relu=  relu_forward(Layer1)\n",
        "        Layer2,cache2=fc_forward(ReLu_layer, self.params['W2'], self.params['b2'])\n",
        "        scores=Layer2\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss,loss_gradient=softmax_loss(scores, y)\n",
        "        dx2,grads['W2'],grads['b2']=fc_backward(loss_gradient,cache2)\n",
        "        dx_relu=relu_backward(dx2,cache_relu)\n",
        "        dx1,grads['W1'],grads['b1']=fc_backward(dx_relu,cache1)\n",
        " \n",
        "        \n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "6123ccb2-ed32-495c-fc02-92f694d15104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "#######################################################################\n",
        "# TODO: Set up model hyperparameters                                  #\n",
        "#######################################################################\n",
        "#In this problem, you need to set up model hyperparameters (hidden dim, learning rate,\n",
        "#lr decay, batch size.)\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim =400, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate =4e-2 ,lr_decay=1.09, num_epochs=10, batch_size=300, print_every=1000)\n",
        "#######################################################################\n",
        "#                         END OF YOUR CODE                            #\n",
        "#######################################################################\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 1330) loss: 2.297339\n",
            "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.133500\n",
            "(Epoch 1 / 10) train acc: 0.410000; val_acc: 0.382800\n",
            "(Epoch 2 / 10) train acc: 0.462000; val_acc: 0.432000\n",
            "(Epoch 3 / 10) train acc: 0.478000; val_acc: 0.451000\n",
            "(Epoch 4 / 10) train acc: 0.517000; val_acc: 0.466500\n",
            "(Epoch 5 / 10) train acc: 0.534000; val_acc: 0.484700\n",
            "(Epoch 6 / 10) train acc: 0.574000; val_acc: 0.491500\n",
            "(Epoch 7 / 10) train acc: 0.570000; val_acc: 0.486500\n",
            "(Iteration 1001 / 1330) loss: 1.158807\n",
            "(Epoch 8 / 10) train acc: 0.586000; val_acc: 0.493200\n",
            "(Epoch 9 / 10) train acc: 0.614000; val_acc: 0.494800\n",
            "(Epoch 10 / 10) train acc: 0.623000; val_acc: 0.505500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3b54f32-40f4-4825-df13-5360e16b10ca"
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7fbe4e02-0fea-4d6a-8901-79c53285d47a"
      },
      "source": [
        "\n",
        "plt.plot(train_acc_history, 'r.--')\n",
        "plt.plot(val_acc_history , 'b.-')\n",
        "plt.title('Accuracy vs Epoch')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show();"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIjDGmCj3/wHJxP1X0UKrOAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUBB_cW6363H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}